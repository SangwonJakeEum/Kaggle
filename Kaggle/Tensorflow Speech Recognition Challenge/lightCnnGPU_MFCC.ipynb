{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightCnnGPU_MFCC\n",
    "urban sound 예제에서 쓰인 feature extraction을 적용시켜 보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import h5py\n",
    "\n",
    "# import keras necessary classes\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#import glob\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1392330768640239658\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 202047488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8074321606245386709\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input/train/audio/_background_noise_/*.wav\n",
      "./input/train/audio/bed/*.wav\n",
      "./input/train/audio/cat/*.wav\n",
      "./input/train/audio/up/*.wav\n",
      "./input/train/audio/down/*.wav\n",
      "./input/train/audio/right/*.wav\n",
      "./input/train/audio/eight/*.wav\n",
      "./input/train/audio/five/*.wav\n"
     ]
    }
   ],
   "source": [
    "parent_dir = './input/train/audio/'\n",
    "tr_sub_dirs = ['_background_noise_','bed','cat','up','down','right','eight','five']\n",
    "#tr_sub_dirs = '_background_noise_ bed bird cat dog down eight five four go happy house left marvin nine no off on one right seven sheila six stop three tree two up wow yes zero'.split()\n",
    "#tr_sub_dirs = ['_background_noise_','bed','bird']\n",
    "file_ext = '*.wav'\n",
    "for l, sub_dir in enumerate(tr_sub_dirs):\n",
    "    print(os.path.join(parent_dir, sub_dir, file_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "#legal_labels = 'bed bird cat dog down eight five four go happy house left marvin nine no off on one right seven sheila six stop three tree two up wow yes zero silence unknown'.split()\n",
    "\n",
    "#src folders\n",
    "root_path = r'.'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'input', 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'input', 'test', 'audio')\n",
    "\n",
    "#train_data_path = os.path.join(root_path, 'input', 'train_test')\n",
    "#test_data_path = os.path.join(root_path, 'input', 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    #print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + window_size\n",
    "        start += (window_size / 2)\n",
    "\n",
    "def extract_features(parent_dir,sub_dirs,file_ext=\"*.wav\",bands = 60, frames = 41):\n",
    "    start_time = time.time();\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    for l, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            sound_clip,s = librosa.load(fn)\n",
    "            #print(fn.split('/'))\n",
    "            #print(fn.split('/')[-2].split('-')[-1])\n",
    "            \n",
    "            label = fn.split('/')[-2].split('-')[-1]\n",
    "            for (start,end) in windows(sound_clip,window_size):\n",
    "                #print(int(start) , int(end))\n",
    "                if(len(sound_clip[int(start):int(end)]) == window_size):\n",
    "                    signal = sound_clip[int(start):int(end)]\n",
    "                    melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                    logspec = librosa.logamplitude(melspec)\n",
    "                    logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                    log_specgrams.append(logspec)\n",
    "                    labels.append(label)\n",
    "                    #print(label)\n",
    "                    #return\n",
    "            \n",
    "    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "        #print(np.array(features).shape)\n",
    "    #print(labels)\n",
    "    print(\"--- %s seconds ---\" % str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tr_features,tr_labels = extract_features(parent_dir,tr_sub_dirs)\n",
    "#print(tr_features.shape, tr_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_features,tr_labels = extract_features(parent_dir,tr_sub_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8416, 60, 41, 2) (8416,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['_background_noise_', '_background_noise_', '_background_noise_',\n",
       "       ..., 'down', 'down', 'down'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tr_features.shape, tr_labels.shape )\n",
    "tr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train = np.array(x_train)\n",
    "#x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(tr_labels)\n",
    "label_index = y_train.columns.values\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()\n",
    "\n",
    "x_train = tr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       ..., \n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8416, 60, 41, 2) (8416, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 60\n",
    "n2 = 41\n",
    "n3 = 2\n",
    "nclass = len(np.array(legal_labels))-1\n",
    "input_shape = (n1,n2,1)\n",
    "nclass = len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 60, 41, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 60, 41, 2)         8         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 59, 40, 8)         72        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 58, 39, 8)         264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 29, 19, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 29, 19, 8)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 31, 21, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 19, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 27, 17, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 8, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 13, 8, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 6, 14)         3776      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 3, 14)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 3, 14)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               86144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 111,804\n",
      "Trainable params: 111,288\n",
      "Non-trainable params: 516\n",
      "_________________________________________________________________\n",
      "Train on 7574 samples, validate on 842 samples\n",
      "Epoch 1/5\n",
      "7574/7574 [==============================] - 2s 263us/step - loss: 1.2084 - acc: 0.4868 - val_loss: 1.8267 - val_acc: 0.1081\n",
      "Epoch 2/5\n",
      "7574/7574 [==============================] - 1s 156us/step - loss: 0.8318 - acc: 0.6470 - val_loss: 0.9967 - val_acc: 0.5404\n",
      "Epoch 3/5\n",
      "7574/7574 [==============================] - 1s 151us/step - loss: 0.5994 - acc: 0.7678 - val_loss: 1.5941 - val_acc: 0.3468\n",
      "Epoch 4/5\n",
      "7574/7574 [==============================] - 1s 151us/step - loss: 0.4712 - acc: 0.8202 - val_loss: 1.2415 - val_acc: 0.4798\n",
      "Epoch 5/5\n",
      "7574/7574 [==============================] - 1s 149us/step - loss: 0.3931 - acc: 0.8594 - val_loss: 1.1972 - val_acc: 0.5285\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "#model-1\n",
    "print('start')\n",
    "input_shape  = (n1,n2,n3)\n",
    "\n",
    "#nclass = len(np.array(legal_labels))-1\n",
    "#nclass = nclass\n",
    "start_time = time.time();\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "img_1  = ZeroPadding2D((1,1))(img_1)\n",
    "\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu,data_format='channels_first')(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.binary_crossentropy)\n",
    "model.summary()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=2017)\n",
    "#vgg\n",
    "batch_size = 128\n",
    "nb_epoch = 1\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.fit(x_train,y_train,nb_epoch= nb_epoch,batch_size = batch_size , validation_split=0.1)\n",
    "model.fit(x_train, y_train, batch_size=128, validation_data=(x_valid, y_valid), epochs=5, shuffle=True, verbose=1)\n",
    "model.save(os.path.join(model_path, 'lightCnnGPU_MFCC.model'))\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# model prediction\n",
    "이제 모델을 완성했으니 y_test 에서 값을 가져와서 모델을  prediction하는 모델을 만들어 보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"./lightCnnGPU_MFCC.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_demo = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7574, 60, 41, 2)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_demo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60, 41, 2)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape\n",
    "x_demo = np.expand_dims(x_train[0],axis=0)\n",
    "x_demo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0033107 ,  0.01119525,  0.13847438,  0.84701973]], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dir = './input/test/audio_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WITHOUT BATCH \n",
    "def extract_features_test(parent_dir,sub_dirs,file_ext=\"*.wav\",bands = 60, frames = 41):\n",
    "    start_time = time.time();\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    fnames = []\n",
    "\n",
    "  \n",
    "    for fn in glob.glob(os.path.join(test_dir, file_ext)):\n",
    "        #print(fn.split('/')[-1])\n",
    "        sound_clip,s = librosa.load(fn)\n",
    "       \n",
    "        for (start,end) in windows(sound_clip,window_size):\n",
    "            #print(int(start) , int(end))\n",
    "            if(len(sound_clip[int(start):int(end)]) == window_size):\n",
    "                signal = sound_clip[int(start):int(end)]\n",
    "                melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                logspec = librosa.logamplitude(melspec)\n",
    "                logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                log_specgrams.append(logspec)\n",
    "                fnames.append(fn.split('/')[-1])\n",
    "                #labels.append(label)\n",
    "                #print(label)\n",
    "                #return\n",
    "            \n",
    "    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "        #print(np.array(features).shape)\n",
    "    #print(labels)\n",
    "    print(\"--- %s seconds ---\" % str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "    return np.array(features), np.array(fnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BATCH TRYING EXAMPLE \n",
    "def extract_features_test(test_dir,sub_dirs,file_ext=\"*.wav\",bands = 60, frames = 41,batch=16):\n",
    "    start_time = time.time();\n",
    "    i=0\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    \n",
    "    for fn in glob.glob(os.path.join(test_dir, file_ext)):\n",
    "        \n",
    "        sound_clip,s = librosa.load(fn)\n",
    "        if i==0:\n",
    "            log_specgrams = []\n",
    "            labels = []\n",
    "        for (start,end) in windows(sound_clip,window_size):\n",
    "            \n",
    "            if(len(sound_clip[int(start):int(end)]) == window_size):\n",
    "                signal = sound_clip[int(start):int(end)]\n",
    "                melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                logspec = librosa.logamplitude(melspec)\n",
    "                logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                log_specgrams.append(logspec)\n",
    "                \n",
    "                if i==batch:\n",
    "                    i=0\n",
    "                    logspec = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "                    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "                    for i in range(len(features)):\n",
    "                        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "                    print('feature->', np.array(features))\n",
    "                    yield np.array(features), np.array(labels)\n",
    "        if i < batch:\n",
    "                logspec = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "                features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "                for i in range(len(features)):\n",
    "                    features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "                print('feature->', np.array(features))\n",
    "                yield np.array(features), np.array(labels)\n",
    "   \n",
    "    print(\"--- %s seconds ---\" % str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH BATCH \n",
    "def extract_features_test(parent_dir,sub_dirs,file_ext=\"*.wav\",bands = 60, frames = 41,batch = 3):\n",
    "    start_time = time.time();\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    fnames = []\n",
    "    \n",
    "    k = 0\n",
    "    print('os.path->',os.path.join(test_dir, file_ext))\n",
    "  \n",
    "    for fn in glob.glob(os.path.join(test_dir, file_ext)):\n",
    "        print('fn->', fn)\n",
    "        sound_clip,s = librosa.load(fn)\n",
    "        if k==0:\n",
    "            print('i==0')\n",
    "            log_specgrams = []\n",
    "            fnames = []\n",
    "            k = k+1\n",
    "        else:\n",
    "            print('i==else')\n",
    "            k = k+1\n",
    " \n",
    "            for (start,end) in windows(sound_clip,window_size):\n",
    "                #print(int(start) , int(end))\n",
    "                if(len(sound_clip[int(start):int(end)]) == window_size):\n",
    "                    signal = sound_clip[int(start):int(end)]\n",
    "                    melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                    logspec = librosa.logamplitude(melspec)\n",
    "                    logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                    log_specgrams.append(logspec)\n",
    "                    fnames.append(fn.split('/')[-1])\n",
    "\n",
    "            if k==batch:\n",
    "                print('i==batch')\n",
    "                k = 0\n",
    "                log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "                features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "                for i in range(len(features)):\n",
    "                    features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "                print('yielding...',i)\n",
    "                yield np.array(features), np.array(fnames)\n",
    "        \n",
    "        \n",
    "    print(\"--- %s seconds ---\" % str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob\n",
    "#tr_features_test,tr_labels_test = extract_features_test(test_dir,tr_sub_dirs,batch=16)\n",
    "#print(tr_features.shape, tr_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_labels_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_features_test[0].shape\n",
    "#trying = np.expand_dims(tr_features[0], axis=0)\n",
    "#model.predict(trying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "os.path-> ./input/test/audio_test/*.wav\n",
      "fn-> ./input/test/audio_test/clip_0a1d41d64.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a0a9fa8e.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a2c1f0e1.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a1c321ee.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a1daeb27.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a0ba79ac.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0e5bbcd.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a2b4d369.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a1da4f17.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0adcba6.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a0b35e75.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_00a2c3e3d.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0f79193.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a0fc880e.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a2a8c77d.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0c61e04.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a1eb91fb.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a1dd64c1.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0a99fbe.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a0c03051.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a1bba03c.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0aa5a41.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a1b9c579.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a1b6fbc9.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a2b8eb7a.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a0f4536c.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a0aa67a9.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "fn-> ./input/test/audio_test/clip_0a0a60a16.wav\n",
      "i==0\n",
      "fn-> ./input/test/audio_test/clip_0a1def888.wav\n",
      "i==else\n",
      "fn-> ./input/test/audio_test/clip_0a2c26061.wav\n",
      "i==else\n",
      "i==batch\n",
      "yielding... 1\n",
      "(2, 60, 41, 2)\n",
      "--- 0:00:01.666090 seconds ---\n",
      "--- 0:00:01.669548 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#extract feature test\n",
    "import glob\n",
    "new_sample_rate = 8000\n",
    "print('start')\n",
    "start_time = time.time();\n",
    "index = []\n",
    "results = []\n",
    "for  imgs,fnames in extract_features_test(test_dir,sub_dir):\n",
    "    print(imgs.shape)\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "\n",
    "index = [w.replace('./input/test/audio_test/', '') for w in index]\n",
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "\n",
    "df.to_csv(os.path.join(out_path, 'sub_300.csv'), index=False)\n",
    "print(\"--- %s seconds ---\" % str(datetime.timedelta(seconds=(time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
